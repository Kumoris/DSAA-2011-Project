{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def plot_decision_boundary_2d(model, X_2d, y, title=\"\", h=0.02):\n",
    "# #     \"\"\"\n",
    "# #     Visualize decision boundary for a 2D feature space.\n",
    "\n",
    "# #     X_2d: DataFrame or array with exactly 2 columns\n",
    "# #     y   : labels (Series or array)\n",
    "# #     \"\"\"\n",
    "# #     if isinstance(X_2d, pd.DataFrame):\n",
    "# #         X_vals = X_2d.values\n",
    "# #         feature_names = list(X_2d.columns)\n",
    "# #     else:\n",
    "# #         X_vals = X_2d\n",
    "# #         feature_names = [\"Feature 1\", \"Feature 2\"]\n",
    "\n",
    "# #     x_min, x_max = X_vals[:, 0].min() - 0.5, X_vals[:, 0].max() + 0.5\n",
    "# #     y_min, y_max = X_vals[:, 1].min() - 0.5, X_vals[:, 1].max() + 0.5\n",
    "\n",
    "# #     xx, yy = np.meshgrid(\n",
    "# #         np.arange(x_min, x_max, h),\n",
    "# #         np.arange(y_min, y_max, h)\n",
    "# #     )\n",
    "\n",
    "# #     grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "# #     Z = model.predict(grid)\n",
    "# #     Z = Z.reshape(xx.shape)\n",
    "\n",
    "# #     plt.figure(figsize=(7, 5))\n",
    "# #     plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"tab10\")\n",
    "# #     scatter = plt.scatter(\n",
    "# #         X_vals[:, 0], X_vals[:, 1],\n",
    "# #         c=y, cmap=\"tab10\", s=10, edgecolor=\"k\", linewidth=0.1\n",
    "# #     )\n",
    "# #     plt.xlabel(feature_names[0])\n",
    "# #     plt.ylabel(feature_names[1])\n",
    "# #     plt.title(title)\n",
    "# #     plt.tight_layout()\n",
    "# #     plt.show()\n",
    "\n",
    "# from sklearn.base import clone\n",
    "\n",
    "# def plot_decision_boundary_2d(model, X_2d, y, title=\"\", h=0.02):\n",
    "#     \"\"\"\n",
    "#     Visualize decision boundary for a 2D feature space.\n",
    "\n",
    "#     model : 已经训练好的模型（在全特征上没关系）\n",
    "#     X_2d  : 只包含两个特征的 DataFrame 或 array\n",
    "#     y     : 标签\n",
    "#     \"\"\"\n",
    "#     # 克隆一个“同结构但未训练”的模型，只在 2D 特征上重新训练，用于画边界\n",
    "#     model_2d = clone(model)\n",
    "#     model_2d.fit(X_2d, y)\n",
    "\n",
    "#     if isinstance(X_2d, pd.DataFrame):\n",
    "#         X_vals = X_2d.values\n",
    "#         feature_names = list(X_2d.columns)\n",
    "#     else:\n",
    "#         X_vals = X_2d\n",
    "#         feature_names = [\"Feature 1\", \"Feature 2\"]\n",
    "\n",
    "#     x_min, x_max = X_vals[:, 0].min() - 0.5, X_vals[:, 0].max() + 0.5\n",
    "#     y_min, y_max = X_vals[:, 1].min() - 0.5, X_vals[:, 1].max() + 0.5\n",
    "\n",
    "#     xx, yy = np.meshgrid(\n",
    "#         np.arange(x_min, x_max, h),\n",
    "#         np.arange(y_min, y_max, h)\n",
    "#     )\n",
    "\n",
    "#     grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "#     # 用 2D 版本的模型预测\n",
    "#     Z = model_2d.predict(grid)\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"tab10\")\n",
    "#     plt.scatter(\n",
    "#         X_vals[:, 0], X_vals[:, 1],\n",
    "#         c=y, cmap=\"tab10\", s=10, edgecolor=\"k\", linewidth=0.1\n",
    "#     )\n",
    "#     plt.xlabel(feature_names[0])\n",
    "#     plt.ylabel(feature_names[1])\n",
    "#     plt.title(title)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def evaluate_classifier(model, X_tr, y_tr, X_te, y_te, labels_sorted):\n",
    "#     # Train the model\n",
    "#     model.fit(X_tr, y_tr)\n",
    "\n",
    "#     # Construct entire set\n",
    "#     X_all = pd.concat([X_tr, X_te], axis=0)\n",
    "#     y_all = pd.concat([y_tr, y_te], axis=0)\n",
    "\n",
    "#     # Predictions for three splits\n",
    "#     y_pred_train = model.predict(X_tr)\n",
    "#     y_pred_test = model.predict(X_te)\n",
    "#     y_pred_all = model.predict(X_all)\n",
    "\n",
    "#     def report_split(y_true, y_pred, split_name):\n",
    "#         acc = accuracy_score(y_true, y_pred)\n",
    "#         prec = precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "#         rec = recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "#         f1 = f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "#         print(f\"{split_name} — Acc: {acc:.4f}, Prec: {prec:.4f}, Rec: {rec:.4f}, F1: {f1:.4f}\")\n",
    "#         return {\n",
    "#             \"accuracy\": acc,\n",
    "#             \"precision\": prec,\n",
    "#             \"recall\": rec,\n",
    "#             \"f1\": f1\n",
    "#         }\n",
    "\n",
    "#     print(f\"\\n=== {model.__class__.__name__} ===\")\n",
    "#     metrics_train = report_split(y_tr, y_pred_train, \"Train\")\n",
    "#     metrics_test = report_split(y_te, y_pred_test, \"Test\")\n",
    "#     metrics_all = report_split(y_all, y_pred_all, \"All\")\n",
    "\n",
    "#     # Plot confusion matrix for all three splits\n",
    "#     for split_name, y_true, y_pred in [\n",
    "#         (\"Train\", y_tr, y_pred_train),\n",
    "#         (\"Test\", y_te, y_pred_test),\n",
    "#         (\"All\", y_all, y_pred_all),\n",
    "#     ]:\n",
    "#         cm = confusion_matrix(y_true, y_pred, labels=labels_sorted)\n",
    "#         plot_confusion(\n",
    "#             cm,\n",
    "#             labels_sorted,\n",
    "#             title=f\"{model.__class__.__name__} — {split_name} confusion matrix\"\n",
    "#         )\n",
    "\n",
    "#     # Return metrics for three splits\n",
    "#     return {\n",
    "#         \"train\": metrics_train,\n",
    "#         \"test\": metrics_test,\n",
    "#         \"all\": metrics_all\n",
    "#     }, model\n",
    "\n",
    "# labels_sorted = sorted(y.unique())\n",
    "\n",
    "# # Logistic Regression\n",
    "# log_reg = LogisticRegression(\n",
    "#     max_iter=200,\n",
    "#     multi_class=\"multinomial\",\n",
    "#     solver=\"lbfgs\",\n",
    "#     n_jobs=-1,\n",
    "#     random_state=RANDOM_STATE,\n",
    "#     class_weight=\"balanced\"\n",
    "# )\n",
    "# lr_metrics, log_reg_fitted = evaluate_classifier(\n",
    "#     log_reg, X_train_scaled, y_train, X_test_scaled, y_test, labels_sorted\n",
    "# )\n",
    "\n",
    "# # Random Forest\n",
    "# rf = RandomForestClassifier(\n",
    "#     n_estimators=200,\n",
    "#     max_depth=None,\n",
    "#     n_jobs=-1,\n",
    "#     random_state=RANDOM_STATE,\n",
    "#     class_weight=\"balanced\"\n",
    "# )\n",
    "# rf_metrics, rf_fitted = evaluate_classifier(\n",
    "#     rf, X_train_scaled, y_train, X_test_scaled, y_test, labels_sorted\n",
    "# )\n",
    "\n",
    "# # ====== Visualize decision boundaries on a 2D feature slice ======\n",
    "\n",
    "# feature_pair = [\"Elevation\", \"Slope\"]  # Can also choose any of the other two features.\n",
    "\n",
    "# X_train_2d = X_train_scaled[feature_pair]\n",
    "\n",
    "# print(\"\\nDecision boundary — Logistic Regression\")\n",
    "# plot_decision_boundary_2d(\n",
    "#     log_reg_fitted,\n",
    "#     X_train_2d,\n",
    "#     y_train,\n",
    "#     title=\"Logistic Regression — decision boundary (Elevation vs Slope)\"\n",
    "# )\n",
    "\n",
    "# print(\"\\nDecision boundary — Random Forest\")\n",
    "# plot_decision_boundary_2d(\n",
    "#     rf_fitted,\n",
    "#     X_train_2d,\n",
    "#     y_train,\n",
    "#     title=\"Random Forest — decision boundary (Elevation vs Slope)\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5a89ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_decision_boundary_2d(model, X_2d, y, title=\"\", h=0.02):\n",
    "#     \"\"\"\n",
    "#     Visualize decision boundary for a 2D feature space.\n",
    "\n",
    "#     X_2d: DataFrame or array with exactly 2 columns\n",
    "#     y   : labels (Series or array)\n",
    "#     \"\"\"\n",
    "#     if isinstance(X_2d, pd.DataFrame):\n",
    "#         X_vals = X_2d.values\n",
    "#         feature_names = list(X_2d.columns)\n",
    "#     else:\n",
    "#         X_vals = X_2d\n",
    "#         feature_names = [\"Feature 1\", \"Feature 2\"]\n",
    "\n",
    "#     x_min, x_max = X_vals[:, 0].min() - 0.5, X_vals[:, 0].max() + 0.5\n",
    "#     y_min, y_max = X_vals[:, 1].min() - 0.5, X_vals[:, 1].max() + 0.5\n",
    "\n",
    "#     xx, yy = np.meshgrid(\n",
    "#         np.arange(x_min, x_max, h),\n",
    "#         np.arange(y_min, y_max, h)\n",
    "#     )\n",
    "\n",
    "#     grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "#     Z = model.predict(grid)\n",
    "#     Z = Z.reshape(xx.shape)\n",
    "\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"tab10\")\n",
    "#     scatter = plt.scatter(\n",
    "#         X_vals[:, 0], X_vals[:, 1],\n",
    "#         c=y, cmap=\"tab10\", s=10, edgecolor=\"k\", linewidth=0.1\n",
    "#     )\n",
    "#     plt.xlabel(feature_names[0])\n",
    "#     plt.ylabel(feature_names[1])\n",
    "#     plt.title(title)\n",
    "#     plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d639bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Visualization for feature selection, tuning, and model comparison\n",
    "# =========================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ---------- 1. SelectKBest: ANOVA F-score 可视化 ----------\n",
    "\n",
    "selector = lr_fs.named_steps[\"select\"]\n",
    "scores = selector.scores_\n",
    "\n",
    "# 所有特征的 F-score\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(len(scores)), scores)\n",
    "plt.title(\"ANOVA F-values for all features\")\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"F-score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top-k 特征的 F-score\n",
    "top_k_idx = np.argsort(scores)[-fs_k:]\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(fs_k), scores[top_k_idx])\n",
    "plt.title(f\"Top {fs_k} selected features by ANOVA F-score\")\n",
    "plt.xlabel(\"Selected feature rank (by score)\")\n",
    "plt.ylabel(\"F-score\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------- 2. Random Forest GridSearchCV 结果热力图 ----------\n",
    "\n",
    "cv_results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "heatmap_data = cv_results.pivot_table(\n",
    "    values=\"mean_test_score\",\n",
    "    index=\"param_max_depth\",\n",
    "    columns=\"param_max_features\",\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", cmap=\"viridis\")\n",
    "plt.title(\"Random Forest GridSearchCV — Mean CV Accuracy\")\n",
    "plt.xlabel(\"max_features\")\n",
    "plt.ylabel(\"max_depth\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------- 3. 不同模型的 3-fold CV accuracy 对比 ----------\n",
    "\n",
    "cv_summary = []\n",
    "for name, model in cv_models.items():\n",
    "    scores = cross_val_score(model, X_cv, y_cv, cv=3,\n",
    "                             scoring=\"accuracy\", n_jobs=-1)\n",
    "    cv_summary.append({\n",
    "        \"Model\": name,\n",
    "        \"MeanAcc\": scores.mean(),\n",
    "        \"StdAcc\": scores.std()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_summary)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=cv_df, x=\"Model\", y=\"MeanAcc\")\n",
    "plt.errorbar(\n",
    "    x=range(len(cv_df)),\n",
    "    y=cv_df[\"MeanAcc\"],\n",
    "    yerr=cv_df[\"StdAcc\"],\n",
    "    fmt=\"none\", ecolor=\"black\", capsize=4\n",
    ")\n",
    "plt.title(\"3-fold CV Accuracy on 20k Subset\")\n",
    "plt.ylabel(\"Mean CV Accuracy\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---------- 4. 各模型 Test accuracy 对比 ----------\n",
    "\n",
    "test_scores = {\n",
    "    \"SelectKBest + LogReg\": fs_acc,\n",
    "    \"GradientBoosting\": gb_acc,\n",
    "    \"HistGradientBoosting\": hgb_acc,\n",
    "    \"Best RandomForest\": best_rf_test_acc,\n",
    "    \"LinearSVM (30k)\": svm_test_acc\n",
    "}\n",
    "\n",
    "test_df = pd.DataFrame({\n",
    "    \"Model\": list(test_scores.keys()),\n",
    "    \"TestAcc\": list(test_scores.values())\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=test_df, x=\"Model\", y=\"TestAcc\")\n",
    "plt.title(\"Test Accuracy Comparison Across Models\")\n",
    "plt.ylabel(\"Test Accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1568887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试用t-SNE来展示降维结果，但效果不是很好\n",
    "\n",
    "\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# def clustering_report(y_true, y_pred, X_embedded=None, title_prefix=\"\"):\n",
    "#     scores = {\n",
    "#         \"ARI\": adjusted_rand_score(y_true, y_pred),\n",
    "#         \"NMI\": normalized_mutual_info_score(y_true, y_pred),\n",
    "#         \"FMI\": fowlkes_mallows_score(y_true, y_pred),\n",
    "#     }\n",
    "#     # Internal metrics 用高维特征更合理，这里传 cluster_sample\n",
    "#     if X_embedded is not None:\n",
    "#         scores[\"Silhouette\"] = silhouette_score(X_embedded, y_pred)\n",
    "#         scores[\"Calinski-Harabasz\"] = calinski_harabasz_score(X_embedded, y_pred)\n",
    "#         scores[\"Davies-Bouldin\"] = davies_bouldin_score(X_embedded, y_pred)\n",
    "#     else:\n",
    "#         scores[\"Silhouette\"] = np.nan\n",
    "#         scores[\"Calinski-Harabasz\"] = np.nan\n",
    "#         scores[\"Davies-Bouldin\"] = np.nan\n",
    "\n",
    "#     print(f\"\\n{title_prefix} clustering metrics:\")\n",
    "#     for k, v in scores.items():\n",
    "#         print(f\"  {k:18s}: {v:.4f}\")\n",
    "#     return scores\n",
    "\n",
    "\n",
    "# def plot_clusters_2d(X_2d, labels, title=\"Clusters\", palette=\"tab10\", savepath=None):\n",
    "#     fig, ax = plt.subplots(figsize=(7, 5))\n",
    "#     sns.scatterplot(\n",
    "#         x=X_2d[:, 0], y=X_2d[:, 1],\n",
    "#         hue=labels, palette=palette,\n",
    "#         s=10, linewidth=0, ax=ax, legend=False\n",
    "#     )\n",
    "#     ax.set_title(title)\n",
    "#     ax.set_xlabel(\"t-SNE 1\")\n",
    "#     ax.set_ylabel(\"t-SNE 2\")\n",
    "#     plt.tight_layout()\n",
    "#     if savepath is not None:\n",
    "#         os.makedirs(os.path.dirname(savepath), exist_ok=True)\n",
    "#         fig.savefig(savepath, format=\"pdf\", dpi=300)\n",
    "#     plt.show()\n",
    "#     plt.close(fig)\n",
    "\n",
    "\n",
    "# ====== Clustering on a sample ======\n",
    "cluster_sample = X_train_scaled.sample(n=15000, random_state=RANDOM_STATE)\n",
    "cluster_labels_true = y_train.loc[cluster_sample.index]\n",
    "\n",
    "# 2D embedding for visualization via t-SNE (nonlinear)\n",
    "tsne_2d = TSNE(\n",
    "    n_components=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    perplexity=30,\n",
    "    learning_rate=200,\n",
    "    init=\"pca\"\n",
    ")\n",
    "cluster_2d_tsne = tsne_2d.fit_transform(cluster_sample)\n",
    "\n",
    "n_clusters = 7  # matches true classes\n",
    "\n",
    "# --- K-Means ---\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=RANDOM_STATE, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(cluster_sample)\n",
    "# internal metrics 用高维 cluster_sample\n",
    "kmeans_scores = clustering_report(cluster_labels_true, kmeans_labels, cluster_sample, \"K-Means\")\n",
    "\n",
    "plot_clusters_2d(\n",
    "    cluster_2d_tsne, kmeans_labels,\n",
    "    title=\"K-Means clusters (t-SNE 2D)\",\n",
    "    savepath=\"./Figures/kmeans_tsne2d.pdf\"\n",
    ")\n",
    "\n",
    "# --- Gaussian Mixture ---\n",
    "gmm = GaussianMixture(n_components=n_clusters, covariance_type=\"full\", random_state=RANDOM_STATE)\n",
    "gmm_labels = gmm.fit_predict(cluster_sample)\n",
    "gmm_scores = clustering_report(cluster_labels_true, gmm_labels, cluster_sample, \"GMM\")\n",
    "\n",
    "plot_clusters_2d(\n",
    "    cluster_2d_tsne, gmm_labels,\n",
    "    title=\"GMM clusters (t-SNE 2D)\",\n",
    "    savepath=\"./Figures/gmm_tsne2d.pdf\"\n",
    ")\n",
    "\n",
    "cluster_compare = pd.DataFrame([kmeans_scores, gmm_scores], index=[\"K-Means\", \"GMM\"])\n",
    "print(\"Clustering Metrics Comparison\")\n",
    "display(\n",
    "    cluster_compare\n",
    "    .style\n",
    "    .set_caption(\"Comparison of K-Means and GMM\")\n",
    "    .format(\"{:.4f}\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
